{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSLab Homework 1 - Data Science with CO2\n",
    "\n",
    "## Hand-in Instructions\n",
    "\n",
    "- __Due: 23.03.2021 23h59 CET__\n",
    "- `git push` your final verion to the master branch of your group's Renku repository before the due\n",
    "- check if `Dockerfile`, `environment.yml` and `requirements.txt` are properly written\n",
    "- add necessary comments and discussion to make your codes readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carbosense\n",
    "\n",
    "The project Carbosense establishes a uniquely dense CO2 sensor network across Switzerland to provide near-real time information on man-made emissions and CO2 uptake by the biosphere. The main goal of the project is to improve the understanding of the small-scale CO2 fluxes in Switzerland and concurrently to contribute to a better top-down quantification of the Swiss CO2 emissions. The Carbosense network has a spatial focus on the City of Zurich where more than 50 sensors are deployed. Network operations started in July 2017.\n",
    "\n",
    "<img src=\"http://carbosense.wdfiles.com/local--files/main:project/CarboSense_MAP_20191113_LowRes.jpg\" width=\"500\">\n",
    "\n",
    "<img src=\"http://carbosense.wdfiles.com/local--files/main:sensors/LP8_ZLMT_3.JPG\" width=\"156\">  <img src=\"http://carbosense.wdfiles.com/local--files/main:sensors/LP8_sensor_SMALL.jpg\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of the homework\n",
    "\n",
    "In this homework, we will curate a set of **CO2 measurements**, measured from cheap but inaccurate sensors, that have been deployed in the city of Zurich from the Carbosense project. The goal of the exercise is twofold: \n",
    "\n",
    "1. Learn how to deal with real world sensor timeseries data, and organize them efficiently using python dataframes.\n",
    "\n",
    "2. Apply data science tools to model the measurements, and use the learned model to process them (e.g., detect drifts in the sensor measurements). \n",
    "\n",
    "The sensor network consists of 46 sites, located in different parts of the city. Each site contains three different sensors measuring (a) **CO2 concentration**, (b) **temperature**, and (c) **humidity**. Beside these measurements, we have the following additional information that can be used to process the measurements: \n",
    "\n",
    "1. The **altitude** at which the CO2 sensor is located, and the GPS coordinates (latitude, longitude).\n",
    "\n",
    "2. A clustering of the city of Zurich in 17 different city **zones** and the zone in which the sensor belongs to. Some characteristic zones are industrial area, residential area, forest, glacier, lake, etc.\n",
    "\n",
    "## Prior knowledge\n",
    "\n",
    "The average value of the CO2 in a city is approximately 400 ppm. However, the exact measurement in each site depends on parameters such as the temperature, the humidity, the altitude, and the level of traffic around the site. For example, sensors positioned in high altitude (mountains, forests), are expected to have a much lower and uniform level of CO2 than sensors that are positioned in a business area with much higher traffic activity. Moreover, we know that there is a strong dependence of the CO2 measurements, on temperature and humidity.\n",
    "\n",
    "Given this knowledge, you are asked to define an algorithm that curates the data, by detecting and removing potential drifts. **The algorithm should be based on the fact that sensors in similar conditions are expected to have similar measurements.** \n",
    "\n",
    "## To start with\n",
    "\n",
    "The following csv files in the `../data/carbosense-raw/` folder will be needed: \n",
    "\n",
    "1. `CO2_sensor_measurements.csv`\n",
    "    \n",
    "   __Description__: It containts the CO2 measurements `CO2`, the name of the site `LocationName`, a unique sensor identifier `SensorUnit_ID`, and the time instance in which the measurement was taken `timestamp`.\n",
    "    \n",
    "2. `temperature_humidity.csv`\n",
    "\n",
    "   __Description__: It contains the temperature and the humidity measurements for each sensor identifier, at each timestamp `Timestamp`. For each `SensorUnit_ID`, the temperature and the humidity can be found in the corresponding columns of the dataframe `{SensorUnit_ID}.temperature`, `{SensorUnit_ID}.humidity`.\n",
    "    \n",
    "3. `sensor_metadata.csv`\n",
    "\n",
    "   __Description__: It contains the name of the site `LocationName`, the zone index `zone`, the altitude in meters `altitude`, the longitude `lon`, and the latitude `lat`. \n",
    "\n",
    "Import the following python packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART I: Handling time series with pandas (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) **8/10**\n",
    "\n",
    "Merge the `CO2_sensor_measurements.csv`, `temperature_humidity.csv`, and `sensors_metadata.csv`, into a single dataframe. \n",
    "\n",
    "* The merged dataframe contains:\n",
    "    - index: the time instance `timestamp` of the measurements\n",
    "    - columns: the location of the site `LocationName`, the sensor ID `SensorUnit_ID`, the CO2 measurement `CO2`, the `temperature`, the `humidity`, the `zone`, the `altitude`, the longitude `lon` and the latitude `lat`.\n",
    "\n",
    "| timestamp | LocationName | SensorUnit_ID | CO2 | temperature | humidity | zone | altitude | lon | lat |\n",
    "|:---------:|:------------:|:-------------:|:---:|:-----------:|:--------:|:----:|:--------:|:---:|:---:|\n",
    "|    ...    |      ...     |      ...      | ... |     ...     |    ...   |  ... |    ...   | ... | ... |\n",
    "\n",
    "\n",
    "\n",
    "* For each measurement (CO2, humidity, temperature), __take the average over an interval of 30 min__. \n",
    "\n",
    "* If there are missing measurements, __interpolate them linearly__ from measurements that are close by in time.\n",
    "\n",
    "__Hints__: The following methods could be useful\n",
    "\n",
    "1. ```python \n",
    "pandas.DataFrame.resample()\n",
    "``` \n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.resample.html\n",
    "    \n",
    "2. ```python\n",
    "pandas.DataFrame.interpolate()\n",
    "```\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.interpolate.html\n",
    "    \n",
    "3. ```python\n",
    "pandas.DataFrame.mean()\n",
    "```\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.mean.html\n",
    "    \n",
    "4. ```python\n",
    "pandas.DataFrame.append()\n",
    "```\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.append.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and check basic characteritics of raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git lfs pull #import lfs files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '../data/carbosense-raw'\n",
    "\n",
    "co2_measurements = os.path.join(root, 'CO2_sensor_measurements.csv')\n",
    "df_co2_raw = pd.read_csv(co2_measurements, sep='\\t')\n",
    "\n",
    "humidity_measurements = os.path.join(root, 'temperature_humidity.csv')\n",
    "df_humidity_raw = pd.read_csv(humidity_measurements, sep='\\t')\n",
    "\n",
    "sensors_meta = os.path.join(root, 'sensors_metadata.csv')\n",
    "df_sensor_raw = pd.read_csv(sensors_meta, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_co2_raw.head(3))\n",
    "df_co2_raw.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_humidity_raw.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_humidity_raw.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_sensor_raw.head(3))\n",
    "df_sensor_raw.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process temperature_humidity.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the list of all the SensorUnit_ID\n",
    "ID_list=df_co2_raw.SensorUnit_ID.unique()\n",
    "\n",
    "\n",
    "#check that the columns are a succesion of temperature and humidity\n",
    "#and also check that two successive temperature->humidity are associated to the same sensorUnit_id\n",
    "#and that the sensorUnit_ID exists in the table df_co2_raw\n",
    "prev_number=0\n",
    "for idx,s in enumerate(df_humidity_raw.columns[1:].values):\n",
    "    if idx%2==0:\n",
    "        prev_number=int(s.split('.')[0])\n",
    "        assert prev_number in ID_list\n",
    "        assert s.split('.')[1]==\"temperature\"\n",
    "    else:\n",
    "        assert prev_number==int(s.split('.')[0])\n",
    "        assert s.split('.')[1]==\"humidity\"\n",
    "\n",
    "\n",
    "#generate a DatetimeIndex from the columns Timestamp\n",
    "time_index=pd.to_datetime(df_humidity_raw['Timestamp'])\n",
    "\n",
    "#set DatetimeIndex, interpolate, and resample every 30minutes\n",
    "df_humidity_30MinAvg=df_humidity_raw\\\n",
    "                        .set_index(time_index)\\\n",
    "                        .interpolate(method='time', axis=0)\\\n",
    "                        .resample('30T').mean()\n",
    "    \n",
    "#verify that there are no null values remaining\n",
    "assert not df_humidity_30MinAvg.isnull().any().any()\n",
    "\n",
    "#generate a DatetimeIndex from the columns Timestamp\n",
    "_30Min_time_index=df_humidity_30MinAvg.index\n",
    "\n",
    "#create list of data frames\n",
    "df_list = np.empty(ID_list.size,dtype=\"object\")\n",
    "\n",
    "for i,sensor_id in enumerate(ID_list):#for each sensor\n",
    "    \n",
    "    #create the dataframe for one particular sensor\n",
    "    df = pd.DataFrame(data={'timestamp':     _30Min_time_index,\n",
    "                            'SensorUnit_ID': [sensor_id]*df_humidity_30MinAvg.shape[0],\n",
    "                            'humidity':      df_humidity_30MinAvg[str(sensor_id)+\".humidity\"].values, \n",
    "                            'temperature':   df_humidity_30MinAvg[str(sensor_id)+\".temperature\"].values})\n",
    "    \n",
    "    df_list[i]=df#add it to the list\n",
    "\n",
    "#concatenate all dataframe for each sensor into one\n",
    "df_humi_temp=pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "#check that the size corresponds and therefore the concatenation is good as well\n",
    "assert df_humi_temp.shape[0]==df_humidity_30MinAvg.shape[0]*ID_list.size\n",
    "\n",
    "df_humi_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process CO2_sensor_measurements.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list of data frames\n",
    "df_list = np.empty(ID_list.size,dtype=\"object\")\n",
    "\n",
    "for i,sensor_id in enumerate(ID_list):#for each sensor\n",
    "    \n",
    "    #isolate one sensor\n",
    "    one_particular_sensor=df_co2_raw[df_co2_raw[\"SensorUnit_ID\"]==sensor_id]\n",
    "    \n",
    "    #get the LocationName\n",
    "    locationName=one_particular_sensor[\"LocationName\"].values[0]\n",
    "    \n",
    "    #create the dataframe for one particular sensor\n",
    "    #interpolate,resample and interpolate again in case\n",
    "    #there were no data over a 30min interval\n",
    "    df = pd.DataFrame(data={'CO2': one_particular_sensor[\"CO2\"].values},\n",
    "                 index=pd.to_datetime(one_particular_sensor[\"timestamp\"]))\\\n",
    "            .interpolate(method='time', axis=0).resample('30T').mean()\\\n",
    "            .interpolate(method='time', axis=0)\n",
    "    df=df.assign(SensorUnit_ID=[sensor_id]*df.shape[0],LocationName=[locationName]*df.shape[0])\n",
    "    \n",
    "    df_list[i]=df#add it to the list\n",
    "\n",
    "#concatenate each sensor's dataframe into one general dataframe\n",
    "df_co2=pd.concat(df_list)\n",
    "\n",
    "#verify that there are no null values\n",
    "assert not df_co2.isnull().any().any()\n",
    "\n",
    "df_co2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join the 3 tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge the CO2 data with the sensor metadata\n",
    "#and then merge with the temperature and humidity\n",
    "data=df_humi_temp.merge(pd.merge(df_co2,df_sensor_raw,on=\"LocationName\",how='left').set_index(df_co2.index),\n",
    "                   on=[\"timestamp\",\"SensorUnit_ID\"])\\\n",
    "                .set_index(\"timestamp\")#set the timestamp as index\n",
    "\n",
    "#check if no value is missing\n",
    "assert not data.isnull().any().any()\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) **2/10** \n",
    "\n",
    "Export the curated and ready to use timeseries to a csv file, and properly push the merged csv to Git LFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Don't run it, I've already done it\n",
    "'''\n",
    "data.to_csv(\"processed_data.csv\")\n",
    "\n",
    "!git lfs status\n",
    "!git lfs track \"processed_data.csv\"\n",
    "!git add .gitattributes\n",
    "!git add processed_data.csv\n",
    "!git commit -m\"added processed_data.csv into lfs\"\n",
    "!git push\n",
    "!git lfs status\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART II: Data visualization (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) **5/15** \n",
    "Group the sites based on their altitude, by performing K-means clustering. \n",
    "- Find the optimal number of clusters using the [Elbow method](https://en.wikipedia.org/wiki/Elbow_method_(clustering)). \n",
    "- Wite out the formula of metric you use for Elbow curve. \n",
    "- Perform clustering with the optimal number of clusters and add an additional column `altitude_cluster` to the dataframe of the previous question indicating the altitude cluster index. \n",
    "- Report your findings.\n",
    "\n",
    "__Note__: [Yellowbrick](http://www.scikit-yb.org/) is a very nice Machine Learning Visualization extension to scikit-learn, which might be useful to you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git lfs pull #import lfs files <- just in case you did not run Part I\n",
    "\n",
    "data = pd.read_csv('processed_data.csv', sep=',')\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#isolate pairs of locations with their altitude\n",
    "pairs_location_altitude=data[[\"LocationName\",\"altitude\"]].drop_duplicates().reset_index().drop(columns=\"index\")#.values\n",
    "\n",
    "#check that we found every locations\n",
    "assert pairs_location_altitude.shape[0]==ID_list.size\n",
    "\n",
    "print(\"5 firsts:\\n\",\n",
    "pairs_location_altitude[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "#altitude list\n",
    "altitudes=pairs_location_altitude[\"altitude\"].values.reshape(-1,1)\n",
    "\n",
    "# Instantiate the clustering model and visualizer\n",
    "model = KMeans()\n",
    "visualizer = KElbowVisualizer(model, k=(1,ID_list.size//2))\n",
    "\n",
    "visualizer.fit(altitudes)        # Fit the data to the visualizer\n",
    "visualizer.show()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As suggested by the graph, 4 clusters is indeed a good choice\n",
    "\n",
    "\n",
    "The scoring method used by the model is defined as the sum of squared distances between each observation and its closest centroid\n",
    "\n",
    "\n",
    "i.e. the formula is $$\\sum_{c \\in Cluster\\_Centers}\\sum_{a \\in Altitudes\\_List}||a-c||^2 \\cdot \\mathbb{1}_{\\text{c is the closet center to a}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=4)\n",
    "\n",
    "#get the cluster index for each altitude\n",
    "labels=kmeans.fit_predict(altitudes)\n",
    "\n",
    "#associate the cluster id to each sensor\n",
    "pairs_location_altitude[\"altitude_cluster\"]=labels\n",
    "\n",
    "#merge with the altitude cluster by merging on the LocationName\n",
    "data=data.merge(pairs_location_altitude[[\"LocationName\",\"altitude_cluster\"]],on=\"LocationName\",how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the altitude and color them by cluster centers\n",
    "px.scatter(pairs_location_altitude.sort_values(by=\"altitude\").reset_index(),y=\"altitude\",color=\"altitude_cluster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are about 2 distinct clusters and 1 outlier. The lower cluster span over a large range of values and can be split in two to sufficiently lower the score metric which resulted into 3 clusters and 1 outlier for a total of 4 clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) **4/15** \n",
    "\n",
    "Use `plotly` (or other similar graphing libraries) to create an interactive plot of the monthly median CO2 measurement for each site with respect to the altitude. \n",
    "\n",
    "Add proper title and necessary hover information to each point, and give the same color to stations that belong to the same altitude cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The data is over only one month\n",
    "px.scatter(data[[\"CO2\",\"SensorUnit_ID\",\"altitude\",\"altitude_cluster\"]].set_index(pd.to_datetime(data[\"timestamp\"]))\\\n",
    "                                                                      .groupby(\"SensorUnit_ID\").resample('1M').median()\\\n",
    "                                                                      .droplevel(\"SensorUnit_ID\")\n",
    "           ,x=\"altitude\",y=\"CO2\",color=\"altitude_cluster\",title=\"Monthly median CO2 with respect to the sensor altitude\"\n",
    "           ,hover_name=\"SensorUnit_ID\", hover_data={'CO2':':.1f',\n",
    "                                                    'altitude':':.1f',\n",
    "                                                    'altitude_cluster':False})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) **6/15**\n",
    "\n",
    "Use `plotly` (or other similar graphing libraries) to plot an interactive time-varying density heatmap of the mean daily CO2 concentration for all the stations. Add proper title and necessary hover information.\n",
    "\n",
    "__Hints:__ Check following pages for more instructions:\n",
    "- [Animations](https://plotly.com/python/animations/)\n",
    "- [Density Heatmaps](https://plotly.com/python/mapbox-density-heatmaps/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep all useful information to print on hoover and aggregate the data by day mean\n",
    "agg_data = data[['SensorUnit_ID', 'CO2', 'lat', 'lon', 'LocationName', 'zone', 'altitude', 'humidity', 'temperature']]\\\n",
    "        .set_index(pd.to_datetime(data[\"timestamp\"])).groupby('SensorUnit_ID').resample('1D').mean().droplevel('SensorUnit_ID').reset_index()\n",
    "agg_data['timestamp'] = agg_data['timestamp'].apply(lambda x: x.strftime(\"%m/%d/%Y\"))\n",
    "\n",
    "fig = px.density_mapbox(agg_data, lat='lat', lon='lon', z='CO2', zoom=11,\\\n",
    "                        animation_frame='timestamp', animation_group='SensorUnit_ID',\\\n",
    "                        title='Time-varying density heatmap of mean daily CO2 concentration in the stations',\\\n",
    "                        height=700, hover_name='SensorUnit_ID', hover_data={'CO2':':.1f',\n",
    "                                                    'altitude':':.1f',\n",
    "                                                    'humidity':':.1f',\n",
    "                                                    'temperature':':.1f',\n",
    "                                                    'zone':':d'})\n",
    "fig.update_layout(mapbox_style='stamen-terrain', margin={\"r\":0,\"t\":30,\"l\":0,\"b\":0})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART III: Model fitting for data curation (35 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) **2/35**\n",
    "\n",
    "The domain experts in charge of these sensors report that one of the CO2 sensors `ZSBN` is exhibiting a drift on Oct. 24. Verify the drift by visualizing the CO2 concentration of the drifting sensor and compare it with some other sensors from the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# covert the 'timestamp' column to datatime type\n",
    "data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "\n",
    "# create pd.series for conditions on the Location Name and Period\n",
    "ZSBN = data['LocationName']=='ZSBN'\n",
    "October = data['timestamp'].apply(lambda x: x.month) == 10\n",
    "\n",
    "# collect all the useful information on the ZSBN sensor\n",
    "ID = data[ZSBN]['SensorUnit_ID'].unique()[0]\n",
    "lat = data[ZSBN]['lat'].unique()\n",
    "lon = data[ZSBN]['lon'].unique()\n",
    "altitude = data[ZSBN]['altitude_cluster'].unique()\n",
    "zone = data[ZSBN]['zone'].unique()\n",
    "\n",
    "\n",
    "# create pd.series for conditions on the Lat, Lon and Zone and altitude to define similar sensors\n",
    "\n",
    "# We define 'similar sensors' 2 sensors:\n",
    "# - in the same cluster of altitude \n",
    "# - in the same geographical area (similar lon and lon) or zone\n",
    "\n",
    "close_lat = data['lat'].apply(lambda x: abs(x-lat) < 0.032 )\n",
    "close_lon = data['lon'].apply(lambda x: abs(x-lon) < 0.032 )\n",
    "same_zone = data['zone'].apply(lambda x: abs(x-zone) == 0 )\n",
    "close_alt = data['altitude_cluster'].apply(lambda x: abs(x-altitude) == 0 )\n",
    "\n",
    "# plot\n",
    "fig = px.line(data[October & (same_zone | (close_lon & close_lat)) & close_alt],\n",
    "            x='timestamp',\n",
    "            y='CO2',\n",
    "            color='LocationName',\n",
    "            labels={\n",
    "                'timestamp':'Time',\n",
    "                'CO2':'CO2 (ppm)',\n",
    "                'LocationName':'Sensor'\n",
    "            }, \n",
    "            title='CO2 Level (ppm) recorded during October 2017 in ZSBN by sensor ID '+str(ID)+' compared with similar sensors'\n",
    "           )\n",
    "fig.update_xaxes(\n",
    "    dtick=24*60*60*1000\n",
    ")\n",
    "fig.add_vline(\n",
    "    x='2017-10-24', line_dash='dash'\n",
    ")\n",
    "fig.update_layout(\n",
    "\n",
    "    hovermode='x unified'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) **8/35**\n",
    "\n",
    "The domain experts ask you if you could reconstruct the CO2 concentration of the drifting sensor had the drift not happened. You decide to:\n",
    "- Fit a linear regression model to the CO2 measurements of the site, by considering as features the covariates not affected by the malfunction (such as temperature and humidity)\n",
    "- Create an interactive plot with `plotly` (or other similar graphing libraries):\n",
    "    - the actual CO2 measurements\n",
    "    - the values obtained by the prediction of the linear model for the entire month of October\n",
    "    - the __confidence interval__ obtained from cross validation\n",
    "- What do you observe? Report your findings.\n",
    "\n",
    "__Note:__ Cross validation on time series is different from that on other kinds of datasets. The following diagram illustrates the series of training sets (in orange) and validation sets (in blue). For more on time series cross validation, there are a lot of interesting articles available online. scikit-learn provides a nice method [`sklearn.model_selection.TimeSeriesSplit`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html).\n",
    "\n",
    "![ts_cv](https://player.slideplayer.com/86/14062041/slides/slide_28.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the datasets\n",
    "X = data[ZSBN & October].set_index('timestamp')[['humidity','temperature']]\n",
    "y = data[ZSBN & October].set_index('timestamp')['CO2']\n",
    "X_train = X[ X.index.day<24 ]\n",
    "y_train = y[ X.index.day<24 ]\n",
    "\n",
    "period = data[ZSBN & October]['timestamp']\n",
    "\n",
    "\n",
    "# Prediction\n",
    "linear_model = sk.linear_model.LinearRegression()\n",
    "linear_model.fit(X_train,y_train)\n",
    "y_pred = linear_model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_stdev(X_train, y_train=y_train, k=100):\n",
    "    '''\n",
    "        Estimate Standard Deviation of the prediction through Cross Validation\n",
    "        \n",
    "        Input: X_train = features of the training set (pd.Dataframe)\n",
    "               y_train = target of the training set (pd.Series)\n",
    "               k = number of folds (scalar)\n",
    "        Output: mean of the unbiased Standard Error estimator for the prediction of each model in the Cross Validation (scalar)\n",
    "    '''\n",
    "    tscv = sk.model_selection.TimeSeriesSplit(n_splits=k)\n",
    "    stdev = 0\n",
    "    for train_index, test_index in tscv.split(X_train):\n",
    "        X_train_CV, X_test_CV = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        y_train_CV, y_test_CV = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "        linear_model.fit(X_train_CV,y_train_CV)\n",
    "        stdev += np.sqrt( sum((linear_model.predict(X_test_CV) - y_test_CV)**2) / (len(y_test_CV)-1)) \n",
    "    stdev /= k\n",
    "    print('Standard deviation prediction: {}'.format(stdev))\n",
    "    print('95% Confident interval Prediction size: {} ppm'.format(stdev*1.96*2))\n",
    "    \n",
    "    return stdev\n",
    "\n",
    "\n",
    "def plot_predicted_CO2(predicted, stdev, measured=data[ZSBN]['CO2'], period=period):\n",
    "    '''\n",
    "        Plot the CO2 Level (ppm) recorded and predicted (95% CI) in October 2017 in ZSBN\n",
    "        \n",
    "        Input: measured = measured level of CO2 by sensor ID 1031 in ZSBN (pd.Series)\n",
    "               predicted = predicted level of CO2 by sensor ID 1031 in ZSBN (np.ndarray)\n",
    "               stdev = standard deviation of the prediction (scalar)\n",
    "               period = range of time considered (pd.Series)\n",
    "        Output: plot comparing the measured values with the predicted with a 95% Confidence Interval (plot)\n",
    "        \n",
    "    '''\n",
    "    fig = go.Figure([\n",
    "        go.Scatter(\n",
    "            name='Measured CO2',\n",
    "            x=period,\n",
    "            y=measured,\n",
    "            mode='lines',\n",
    "        ),\n",
    "        go.Scatter(\n",
    "            name='Predicted CO2',\n",
    "            x=period,\n",
    "            y=predicted,\n",
    "            mode='lines',\n",
    "        ),  \n",
    "        go.Scatter(\n",
    "            name='Upper Bound',\n",
    "            x=period,\n",
    "            y=y_pred+1.96*stdev, # alpha = 0.975\n",
    "            mode='lines',\n",
    "            marker=dict(color=\"#444\"),\n",
    "            line=dict(width=0),\n",
    "            showlegend=False\n",
    "        ),\n",
    "        go.Scatter(\n",
    "            name='Lower Bound',\n",
    "            x=period,\n",
    "            y=y_pred-1.96*stdev, # alpha = 0.025\n",
    "            marker=dict(color=\"#444\"),\n",
    "            line=dict(width=0),\n",
    "            mode='lines',\n",
    "            fillcolor='rgba(68, 68, 68, 0.3)',\n",
    "            fill='tonexty',\n",
    "            showlegend=False\n",
    "        )\n",
    "    ])\n",
    "    fig.update_xaxes(\n",
    "        dtick=24*60*60*1000\n",
    "    )\n",
    "    fig.add_vline(\n",
    "        x='2017-10-24', line_dash='dash'\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        yaxis_title='CO2 (ppm)',\n",
    "        title='Comparison CO2 Level (ppm) recorded and predicted (95% CI) in October 2017 in ZSBN by sensor ID '+str(ID),\n",
    "        hovermode=\"x unified\"\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdev = estimate_stdev(X_train)\n",
    "plot_predicted_CO2(y_pred, stdev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was already showed through the plot in Section 3.a that the sensor 1031 in ZSBN has had a negative drift from 24 October on, and all the days after the values recorded were almost 100ppm smaller than the values recorded by sensors in similar areas. Similarly, a linear model using the humidity and temperature of October recorded by the same sensor (trained only until the 23 of October), shows that the expected (predicted) values of the CO2 recorded in ZSBN is higher (see red line in the graph above) with a quite strict 95% Confidence Interval (see green area in the graph above). It seems therefore that just humidity and temperature can already express a quite accurate raw estimate of the CO2 level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) **10/35**\n",
    "\n",
    "In your next attempt to solve the problem, you decide to exploit the fact that the CO2 concentrations, as measured by the sensors __experiencing similar conditions__, are expected to be similar.\n",
    "\n",
    "- Find the sensors sharing similar conditions with `ZSBN`. Explain your definition of \"similar condition\".\n",
    "- Fit a linear regression model to the CO2 measurements of the site, by considering as features:\n",
    "    - the information of provided by similar sensors\n",
    "    - the covariates associated with the faulty sensors that were not affected by the malfunction (such as temperature and humidity).\n",
    "- Create an interactive plot with `plotly` (or other similar graphing libraries):\n",
    "    - the actual CO2 measurements\n",
    "    - the values obtained by the prediction of the linear model for the entire month of October\n",
    "    - the __confidence interval__ obtained from cross validation\n",
    "- What do you observe? Report your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset. New features:\n",
    "# - Humidity and Temperature of the sensor ID 1031 in ZSBN\n",
    "# - CO2 level of all the sensors similar to ID 1031\n",
    "\n",
    "dataset = data[-ZSBN & October & (same_zone | (close_lon & close_lat)) & close_alt][['timestamp','CO2','LocationName']]\n",
    "dataset = dataset.set_index(['timestamp','LocationName']).unstack()['CO2'].dropna(axis='columns')\n",
    "dataset = dataset.join(data[ZSBN].set_index(['timestamp'])[['humidity','temperature']])\n",
    "\n",
    "X = dataset\n",
    "X_train = X[ X.index.day<24 ]\n",
    "\n",
    "# Prediction\n",
    "linear_model.fit(X_train,y_train)\n",
    "y_pred = linear_model.predict(X)\n",
    "\n",
    "\n",
    "# Estimate Standard Deviation of the prediction\n",
    "stdev = estimate_stdev(X_train)\n",
    "\n",
    "# plot\n",
    "plot_predicted_CO2(y_pred, stdev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have a better estimate of the missing data from the ZSBN sensor, we will use the CO2 information from similar sensors. Here, we define a similar sensor to ZSBN as a sensor which is in a similar location. By this, we mean a sensor in close altitude, and location on a map. The latter can be understood either as being in the same geographical zone or being close enough in term of the latitude and longitude.\n",
    "\n",
    "To obtain an estimate of the missing values for the ZSBN sensor, we will use as regressors the humidity and temperature of this sensor, but also all the CO2 level recorded by the similar sensors. The 95% confidence interval is computed using cross validation to estimate the standard deviation and assuming normal error with no sensor bias.\n",
    "\n",
    "We observe on the above figures the predicted CO2 measurement for the ZSBN sensor, compared to the actual measurement. From the 24th October we start seeing faulty observation, which has a negative drift. The linear regression computes a similar looking signal but without the shift. The confidence interval is tighter than the regression method with only humidity and temperature. We can conclude that this prediction provides an acceptable approximation of the CO2 level of the faulty sensor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) **10/35**\n",
    "\n",
    "Now, instead of feeding the model with all features, you want to do something smarter by using linear regression with fewer features.\n",
    "\n",
    "- Start with the same sensors and features as in question c)\n",
    "- Leverage at least two different feature selection methods\n",
    "- Create similar interactive plot as in question c)\n",
    "- Describe the methods you choose and report your findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection 1\n",
    "def F_test_f_selection(X, X_train, y_train=y_train, K=5):\n",
    "    '''\n",
    "    F-test features selector\n",
    "    \n",
    "    Input: X = features of the whole dataset (pd.Dataframe)\n",
    "           X_train = features of the training dataset (pd.Dataframe)\n",
    "           y_train = targets of the training set (pd.Series)\n",
    "           K = number if features to select\n",
    "    Output: X_r = whole dataset with the features selected (pd.Dataframe)\n",
    "            X_train_r = training set with the features selected (pd.Dataframe)\n",
    "    \n",
    "    '''\n",
    "    selector = sk.feature_selection.SelectKBest(sk.feature_selection.f_regression,k=K).fit(X_train,y_train)\n",
    "    F, pvals = sk.feature_selection.f_regression(X_train,y_train)\n",
    "    \n",
    "    relevant_features = selector.get_support(indices=True)\n",
    "    print('t-test featureS selection: selected {} features out of {} ({} discarded) ---> {}'\n",
    "          .format(len(relevant_features), X.shape[1], X.shape[1]-len(relevant_features), list(X.columns[relevant_features])))\n",
    "    X_r = X.iloc[:,relevant_features]\n",
    "    X_train_r = X_train.iloc[:,relevant_features]\n",
    "    return X_r, X_train_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection 2\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def t_test_f_selection(X, X_train, y_train=y_train, threshold = 0.05):\n",
    "    '''\n",
    "    t-test features selector\n",
    "    \n",
    "    Input: X = features of the whole dataset (pd.Dataframe)\n",
    "           X_train = features of the training dataset (pd.Dataframe)\n",
    "           y_train = targets of the training set (pd.Series)\n",
    "           threshold = level of the t-test\n",
    "    Output: X_r = whole dataset with the features selected (pd.Dataframe)\n",
    "            X_train_r = training set with the features selected (pd.Dataframe)\n",
    "    \n",
    "    '''\n",
    "    # train linear model\n",
    "    res = sm.OLS(y_train, X_train_r1).fit()\n",
    "    \n",
    "    relevant_features = res.pvalues <= threshold\n",
    "    print('F-test featureS selection: selected {} features out of {} ({} discarded) ---> {}'\n",
    "          .format(len(relevant_features), X.shape[1], X.shape[1]-len(relevant_features), list(relevant_features.index)))\n",
    "    X_r = X.loc[ :, relevant_features ]\n",
    "    X_train_r = X_train.loc[ :, relevant_features ]\n",
    "    return X_r, X_train_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features selection\n",
    "X_r1, X_train_r1 = F_test_f_selection(X, X_train)\n",
    "X_r2, X_train_r2 = t_test_f_selection(X_r1, X_train_r1)\n",
    "\n",
    "# prediction\n",
    "linear_model.fit(X_train_r2,y_train)\n",
    "y_pred = linear_model.predict(X_r2)\n",
    "\n",
    "# Create 95 Confidence Interval on prediction through Cross Validation\n",
    "stdev = estimate_stdev(X_train_r2)\n",
    "\n",
    "# plot\n",
    "plot_predicted_CO2(y_pred, stdev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we propose to select features with respect to their individual correlation to the target variable (the CO2 emission of the ZSBN sensor) to filter only the most important features using the F-regression method. Then we consider the added value of each feature by evaluating the effect of removing it, in order to discard reduntant features, using the t-test on the coefficients of the linear regression. \n",
    "\n",
    "The F-regression method computes the correlation $\\rho_i$ between each feature $i$ and the target variable, then derives the F-Statistic $F_i$ and it selects only the $K$ variables that have the highest F-Statistic. Our hope is that the features that are highly correlated to the target will be able to predict it accurately. In formula:\n",
    "\n",
    "$$ \\rho_{i}=\\frac{(X[:, i]-\\operatorname{mean}(X[:, i])) *(y-\\operatorname{mean}(y))}{\\operatorname{std}(X[:, i]) * s t d(y)}, \\hspace{10pt} F_{i}=\\frac{\\rho_{i}^{2}}{1-\\rho_{i}^{2}} *(n-2) $$\n",
    "\n",
    "where $X$ is our set of samples, $y$ is our response variable and and $n$ is the number of samples.\n",
    "\n",
    "From this method, we observe that the 5 features that are the most correlated to the response variable are the CO2 observation from the sensors ZDLT, ZECB, ZHRG, ZHRZ and ZUE. It is interesting to observe that the CO2 level of similar sensors is stronger correlated with the answer than the temperature and humidity of the same sensor: this is a great support on the rightness of our definition of similar sensors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second criteria that we choose for features selection is a $t$-test on the coefficients of the trained linear model. In particular, for each coefficient $\\beta_i$ of the linear model we consider the following test:\n",
    "$$ H_{0}: \\beta_i = 0 \\qquad vs \\qquad H_{1}: \\beta_i \\neq 0 $$\n",
    "in other words we are asking if the feature $i$ is a relevant regressor for the model or it could be discarded (setting $\\beta_i = 0$). The test statistic used for this test is: \n",
    "$$ T_0 = \\frac{\\hat{\\beta}_i}{se(\\hat{\\beta}_i)} $$\n",
    "where $\\hat{\\beta}_i$ is the least square estimate of  $\\beta_i$, and $se(\\hat{\\beta}_i)$ is its standard error. The test statistic $T_0$ , follows a $t$ distribution with $(n−2)$ degrees of freedom, where $n$ is the total number of observations. The null hypothesis, $H_0$, is accepted if the calculated value of the test statistic is such that:\n",
    "$$−t_{\\alpha/2,n−2}<T_0<t_{\\alpha/2,n−2}$$\n",
    "Considering $\\alpha = 0.05$ no one sensor is discarded by this second selector and the final features selected remain the CO2 observation from the sensors ZDLT, ZECB, ZHRG, ZHRZ and ZUE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a new liner model only with the features selected we get a new prediction of the CO2 level observed by the ZSBN sensor. This prediction is very similar to that one before feature selection and also the 95% Confidence Interval size is almost the same. This means that the humidity and temperature of the sensor ZSBN are not realy necessary in order to reconstruct its CO2 level time series and that the measurements of the similar sensors are enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e) **5/35**\n",
    "\n",
    "Eventually, you'd like to try something new - __Bayesian Structural Time Series Modelling__ - to reconstruct counterfactual values, that is, what the CO2 measurements of the faulty sensor should have been, had the malfunction not happened on October 24. You will use:\n",
    "- the information of provided by similar sensors - the ones you identified in question c)\n",
    "- the covariates associated with the faulty sensors that were not affected by the malfunction (such as temperature and humidity).\n",
    "\n",
    "To answer this question, you can choose between a Python port of the CausalImpact package (such as https://github.com/dafiti/causalimpact) or the original R version (https://google.github.io/CausalImpact/CausalImpact.html) that you can run in your notebook via an R kernel (https://github.com/IRkernel/IRkernel).\n",
    "\n",
    "Before you start, watch first the [presentation](https://www.youtube.com/watch?v=GTgZfCltMm8) given by Kay Brodersen (one of the creators of the causal impact implementation in R), and this introductory [ipython notebook](http://nbviewer.jupyter.org/github/dafiti/causalimpact/blob/master/examples/getting_started.ipynb) with examples of how to use the python package.\n",
    "\n",
    "- Report your findings:\n",
    "    - Is the counterfactual reconstruction of CO2 measurements significantly different from the observed measurements?\n",
    "    - Can you try to explain the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to run it, the package is already installed\n",
    "# ! pip install pycausalimpact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from causalimpact import CausalImpact\n",
    "import time\n",
    "\n",
    "# The list of covariates we used in question 3.c\n",
    "covariates = ['ZDLT', 'ZECB', 'ZHRG', 'ZHRZ', 'ZUE', 'humidity', 'temperature']\n",
    "covariates_data = []\n",
    "faulty_timestamp = datetime.strptime('24/10/2017 00:00:00', '%d/%m/%Y %H:%M:%S')\n",
    "y = data[(data.LocationName == 'ZSBN')].set_index('timestamp')['CO2']\n",
    "\n",
    "for i, cov in enumerate(covariates):\n",
    "    covariates_data.append(dataset.iloc[:,i])\n",
    "\n",
    "# We construct our dataframe to run our bayesian modeling from the similar sensors, conditions and our faulty censor \n",
    "bayesian_modeling_data = pd.DataFrame({'x0': covariates_data[0], 'x1': covariates_data[1], 'x2': covariates_data[2], 'x3': covariates_data[3],\\\n",
    "                                       'x4': covariates_data[4], 'x5': covariates_data[5], 'x6': covariates_data[6], 'y': y},\\\n",
    "                                      columns=['y', 'x0', 'x1', 'x2', 'x3', 'x4', 'x5','x6'])\n",
    "\n",
    "# Gets the the starting faulty timestamps from index\n",
    "bayesian_modeling_data.reset_index(inplace=True)\n",
    "faulty_timestamp_index = bayesian_modeling_data.timestamp[bayesian_modeling_data.timestamp == faulty_timestamp].index.tolist()\n",
    "\n",
    "timestamps = bayesian_modeling_data.timestamp.tolist()\n",
    "starting_timestamp = timestamps[0]\n",
    "faulty_timestamp = timestamps[faulty_timestamp_index[0]]\n",
    "faulty_timestamp_next = timestamps[faulty_timestamp_index[0]+1]\n",
    "ending_timestamp = timestamps[-1]\n",
    "\n",
    "bayesian_modeling_data.set_index(['timestamp'], inplace = True)\n",
    "\n",
    "pre_period = [starting_timestamp, faulty_timestamp]\n",
    "post_period = [faulty_timestamp_next, ending_timestamp]\n",
    "\n",
    "# Running the model\n",
    "ci = CausalImpact(bayesian_modeling_data, pre_period, post_period)\n",
    "\n",
    "ci.plot(figsize=(18, 4), panels=['original'])\n",
    "print(ci.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see on the first graph above that the predictions are significantly different from the observed measurements as they fall way beyond the confidence intervals of our new model.   \n",
    "Similarly to our previous results, we can observe the predicted CO2 levels for our drifting sensor seem very similar to the actual measurements but shifted up to the inital usual sensor CO2 level. The predictions are also a little smoother than the actual measurements.  \n",
    "This makes sense as even with the drift, the faulty sensor still seems to detect similar CO2 variations as the other similar captors we identified. Maybe some part of the captor got obstructed and the remaining part only received a fraction of the CO2 which would explain why it still follows the general pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That's all, folks!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
